# -*- coding: utf-8 -*-
"""file_type_extraction_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Fy3QFE938WEQEjPxWcab_7f-84dHZzu
"""

pip install pymupdf pdfplumber pytesseract pillow python-docx beautifulsoup4 lxml

import logging
import re
import unicodedata
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

# PDF processing
import fitz                     # PyMuPDF
import pdfplumber
# OCR
import pytesseract
from PIL import Image
# DOCX
from docx import Document
# HTML
from bs4 import BeautifulSoup

# ──────────────────────────────────────────────────────────────────────────────
# Logging setup
# ──────────────────────────────────────────────────────────────────────────────
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("DocumentProcessor")


@dataclass
class DocumentChunk:
    text: str
    metadata: Dict[str, Any]
    chunk_id: int
    page_num: Optional[int] = None


class DocumentProcessor:
    """Processes various document formats into clean, chunked text."""

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        min_chunk_chars: int = 120,
        ocr_lang: str = "eng"
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_chars = min_chunk_chars
        self.ocr_lang = ocr_lang

    def process_file(self, file_path: str) -> List[DocumentChunk]:
        """Main entry point - process file according to its extension."""
        path = Path(file_path)
        if not path.is_file():
            raise FileNotFoundError(f"File not found: {file_path}")

        ext = path.suffix.lower()

        processors = {
            ".pdf": self._process_pdf,
            ".docx": self._process_docx,
            ".doc": self._process_docx,
            ".html": self._process_html,
            ".htm": self._process_html,
        }

        processor = processors.get(ext)
        if processor is None:
            raise ValueError(f"Unsupported file type: {ext}")

        logger.info(f"Processing {ext.upper()} file: {path.name}")
        return processor(path)

    # -------------------------------------------------------------------------
    #                             PDF Processing
    # -------------------------------------------------------------------------

    def _process_pdf(self, path: Path) -> List[DocumentChunk]:
        """Handle both text-based and scanned PDFs."""
        pages = self._extract_pdf_text(path)

        if self._is_likely_scanned_pdf(pages):
            logger.info("Scanned PDF detected → switching to OCR")
            pages = self._ocr_pdf(path)

        pages = self._remove_common_headers_footers(pages)

        all_chunks = []
        for page_info in pages:
            clean_text = self._normalize_text(page_info["text"])
            metadata = {
                "source": str(path),
                "type": "pdf",
                "page": page_info["page"]
            }
            chunks = self._chunk_text(clean_text, metadata, page_info["page"])
            all_chunks.extend(chunks)

        return all_chunks

    def _extract_pdf_text(self, path: Path) -> List[Dict[str, Any]]:
        """Try to extract text + tables using pdfplumber."""
        pages = []
        try:
            with pdfplumber.open(path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    tables = page.extract_tables() or []
                    table_text = "\n".join(
                        " | ".join(str(cell) for cell in row if cell)
                        for table in tables for row in table
                    )
                    pages.append({
                        "page": i + 1,
                        "text": (text + "\n" + table_text).strip()
                    })
        except Exception as e:
            logger.error(f"pdfplumber failed: {e}")
        return pages

    def _is_likely_scanned_pdf(self, pages: List[Dict[str, Any]]) -> bool:
        if not pages:
            return False
        empty_count = sum(1 for p in pages if len(p["text"].strip()) < 50)
        return (empty_count / len(pages)) > 0.7

    def _ocr_pdf(self, path: Path) -> List[Dict[str, Any]]:
        """OCR each page using Tesseract via PyMuPDF rendering."""
        pages = []
        try:
            doc = fitz.open(path)
            max_pages = min(len(doc), 300)  # safety limit

            for i in range(max_pages):
                page = doc[i]
                pix = page.get_pixmap(dpi=220)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                img = img.convert("L")  # grayscale → usually better for OCR

                try:
                    text = pytesseract.image_to_string(
                        img,
                        lang=self.ocr_lang,
                        config='--psm 6 --oem 3'
                    )
                except Exception as e:
                    logger.warning(f"OCR failed on page {i+1}: {e}")
                    text = ""

                pages.append({"page": i + 1, "text": text})

            doc.close()
        except Exception as e:
            logger.error(f"OCR pipeline failed for {path}: {e}")

        return pages

    def _remove_common_headers_footers(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Simple heuristic: remove lines that appear on many pages."""
        if len(pages) < 3:
            return pages

        line_freq = {}
        for page in pages:
            for line in page["text"].splitlines():
                line = line.strip()
                if 10 < len(line) < 60:  # typical header/footer length
                    line_freq[line] = line_freq.get(line, 0) + 1

        threshold = len(pages) * 0.65
        repeated = {line for line, count in line_freq.items() if count >= threshold}

        for page in pages:
            lines = [l for l in page["text"].splitlines() if l.strip() not in repeated]
            page["text"] = "\n".join(lines)

        return pages

    # -------------------------------------------------------------------------
    #                             DOCX Processing
    # -------------------------------------------------------------------------

    def _process_docx(self, path: Path) -> List[DocumentChunk]:
        try:
            doc = Document(path)
            text = "\n".join(para.text for para in doc.paragraphs if para.text.strip())
            clean_text = self._normalize_text(text)

            return self._chunk_text(
                clean_text,
                {"source": str(path), "type": "docx"},
                page_num=None
            )
        except Exception as e:
            logger.error(f"Failed to process DOCX {path}: {e}")
            return []

    # -------------------------------------------------------------------------
    #                             HTML Processing
    # -------------------------------------------------------------------------

    def _process_html(self, path: Path) -> List[DocumentChunk]:
        try:
            html = path.read_text(encoding="utf-8", errors="replace")
            soup = BeautifulSoup(html, "lxml")

            # Remove unwanted elements
            for tag in soup(["script", "style", "nav", "footer", "aside", "head"]):
                tag.decompose()

            main = soup.find("article") or soup.find("main") or soup.body
            text = main.get_text(separator="\n", strip=True) if main else ""

            clean_text = self._normalize_text(text)

            return self._chunk_text(
                clean_text,
                {"source": str(path), "type": "html"},
                page_num=None
            )
        except Exception as e:
            logger.error(f"Failed to process HTML {path}: {e}")
            return []

    # -------------------------------------------------------------------------
    #                           Common Utilities
    # -------------------------------------------------------------------------

    def _normalize_text(self, text: str) -> str:
        """Clean unicode, collapse whitespace, remove most control chars."""
        text = unicodedata.normalize("NFKC", text)
        text = re.sub(r"\s+", " ", text)
        text = re.sub(r"[^\x20-\x7E\n\t]", "", text)  # keep printable + newline/tab
        return text.strip()

    def _chunk_text(
        self,
        text: str,
        metadata: Dict[str, Any],
        page_num: Optional[int]
    ) -> List[DocumentChunk]:
        """Split text into overlapping chunks by sentence boundaries."""
        if not text.strip():
            return []

        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        chunks = []
        current = []
        current_len = 0
        chunk_id = 0

        for sentence in sentences:
            sent_len = len(sentence)

            if current_len + sent_len > self.chunk_size and current:
                chunk_text = " ".join(current).strip()
                if len(chunk_text) >= self.min_chunk_chars:
                    chunks.append(DocumentChunk(
                        text=chunk_text,
                        metadata=metadata.copy(),
                        chunk_id=chunk_id,
                        page_num=page_num
                    ))
                    chunk_id += 1

                # Overlap
                overlap_text = " ".join(current)[-self.chunk_overlap:]
                current = [overlap_text] if overlap_text.strip() else []
                current_len = len(overlap_text)

            current.append(sentence)
            current_len += sent_len

        # Last chunk
        if current:
            chunk_text = " ".join(current).strip()
            if len(chunk_text) >= self.min_chunk_chars:
                chunks.append(DocumentChunk(
                    text=chunk_text,
                    metadata=metadata.copy(),
                    chunk_id=chunk_id,
                    page_num=page_num
                ))

        logger.debug(f"Created {len(chunks)} chunks")
        return chunks


if __name__ == "__main__":
    processor = DocumentProcessor(chunk_size=800, chunk_overlap=150)
    chunks = processor.process_file("/content/example.pdf")
    print(f"Total chunks: {len(chunks)}")