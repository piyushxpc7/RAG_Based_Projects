# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17bwj086XR-HwnuOyBZd-3LLbLU4Y9VyU
"""

!pip install chromadb faiss-cpu sentence-transformers datasets pandas numpy tqdm

def setup_environment():
    """Installs required packages and sets up directories"""
    print("Make sure you have run:")
    print("pip install -q chromadb faiss-cpu sentence-transformers datasets tqdm numpy psutil")

    global os, json, time, np, psutil, tqdm, load_dataset, SentenceTransformer, chromadb, faiss

    import os
    import json
    import time
    import numpy as np
    import psutil
    from tqdm import tqdm
    from datasets import load_dataset
    from sentence_transformers import SentenceTransformer
    import chromadb
    import faiss

    os.makedirs('data', exist_ok=True)
    os.makedirs('chroma_db', exist_ok=True)

    print("Environment setup complete")
    print(f"Available RAM: {psutil.virtual_memory().available / 1e9:.1f} GB")

    return True


setup_environment()

def download_and_clean_wikipedia(num_docs=50000, config="20231101.en"):
    print(f"Loading {num_docs} documents from wikimedia/wikipedia ({config})...")
    dataset = load_dataset("wikimedia/wikipedia", config, split=f"train[:{num_docs}]")

    cleaned_docs = []
    for i, item in enumerate(tqdm(dataset, desc="Cleaning docs")):
        text = item['text'].strip()
        if len(text) < 100:
            continue
        cleaned_docs.append({
            'doc_id': i,
            'title': item['title'],
            'url': item['url'],
            'full_text': text
        })

    print(f"Cleaned: {len(cleaned_docs)} documents")
    with open('data/cleaned_docs.json', 'w') as f:
        json.dump(cleaned_docs, f)
    return cleaned_docs


if not os.path.exists('data/cleaned_docs.json'):
    docs = download_and_clean_wikipedia(num_docs=10000)
else:
    with open('data/cleaned_docs.json') as f:
        docs = json.load(f)
    print(f"Loaded {len(docs)} cleaned documents from cache")

def prepare_documents_for_embedding(docs_list):
    """Extracts texts and creates basic metadata from loaded documents"""
    texts = []
    metadatas = []

    for doc in tqdm(docs_list, desc="Preparing documents"):
        text = doc['full_text']
        texts.append(text)

        length_cat = 'short' if len(text) < 2000 else 'medium' if len(text) < 10000 else 'long'

        metadatas.append({
            'title': doc['title'],
            'url': doc['url'],
            'doc_id': doc['doc_id'],
            'length_category': length_cat
        })

    print(f"Prepared {len(texts)} documents for embedding")
    return texts, metadatas


texts, metadatas = prepare_documents_for_embedding(docs)

def generate_or_load_embeddings(texts_list, model_name="all-MiniLM-L6-v2", batch_size=64):
    """Generates embeddings or loads from cache"""
    embed_file = 'data/embeddings.npy'
    model = SentenceTransformer(model_name)

    if os.path.exists(embed_file):
        print("Loading cached embeddings...")
        embeddings = np.load(embed_file)
    else:
        print(f"Generating embeddings with {model_name} (this may take 10-60 min)...")
        embeddings = model.encode(
            texts_list,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        np.save(embed_file, embeddings)
        print("Embeddings saved to disk")

    dim = embeddings.shape[1]
    print(f"→ Embeddings shape: {embeddings.shape} | Dim: {dim}")
    print(f"→ Memory: {embeddings.nbytes / 1e6:.1f} MB")

    return embeddings, model


embeddings, embedding_model = generate_or_load_embeddings(texts)

import os
import time
import numpy as np
import faiss
import chromadb
from tqdm import tqdm

def create_or_load_indexes(embeddings_array, texts, metadatas, collection_name="wiki_full_docs"):
    """Create or load Chroma + FAISS Flat + FAISS HNSW indexes"""
    os.makedirs("data", exist_ok=True)

    embeddings_array = np.asarray(embeddings_array, dtype=np.float32)
    dim = embeddings_array.shape[1]

    # Chroma
    chroma_client = chromadb.PersistentClient(path="chroma_db")
    try:
        chroma_coll = chroma_client.get_collection(collection_name)
    except:
        chroma_coll = chroma_client.create_collection(name=collection_name)
        chroma_coll.add(
            ids=[f"doc_{i}" for i in range(len(embeddings_array))],
            documents=texts,
            embeddings=embeddings_array.tolist(),
            metadatas=metadatas
        )

    # FAISS Flat (exact cosine)
    flat_file = "data/faiss_flat.index"
    if os.path.exists(flat_file):
        index_flat = faiss.read_index(flat_file)
    else:
        index_flat = faiss.IndexFlatIP(dim)
        index_flat.add(embeddings_array)
        faiss.write_index(index_flat, flat_file)

    # FAISS HNSW (approximate)
    hnsw_file = "data/faiss_hnsw.index"
    if os.path.exists(hnsw_file):
        index_hnsw = faiss.read_index(hnsw_file)
    else:
        # Safe way: create flat IP → wrap in HNSW
        index_hnsw = faiss.IndexHNSWFlat(dim, 32)
        index_hnsw.metric = faiss.METRIC_INNER_PRODUCT
        index_hnsw.hnsw.efConstruction = 200
        index_hnsw.add(embeddings_array)
        faiss.write_index(index_hnsw, hnsw_file)

    index_hnsw.hnsw.efSearch = 64

    return chroma_coll, index_flat, index_hnsw

def quick_benchmark_and_filter(chroma_coll, query_text="machine learning", top_k=5):
    """Simple test query with metadata filtering"""
    q_emb = embedding_model.encode([query_text], normalize_embeddings=True)


    results = chroma_coll.query(
        query_embeddings=q_emb.tolist(),
        n_results=top_k,
        where={"length_category": "long"},
        include=["metadatas", "documents", "distances"]
    )

    print(f"\nTop {top_k} long documents for: '{query_text}'")
    for i, meta in enumerate(results['metadatas'][0]):
        print(f"{i+1}. {meta['title']}  ({meta['length_category']})")
        print(f"   {results['documents'][0][i][:180]}...\n")


chroma_collection, index_flat, index_hnsw = create_or_load_indexes(embeddings, texts, metadatas)
quick_benchmark_and_filter(chroma_collection, "artificial intelligence", top_k=5)